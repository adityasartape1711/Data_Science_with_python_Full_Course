{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt7SQ6RsWmyE",
        "outputId": "9aad7a18-d459-42a6-f594-cc6a31259851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.2/418.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install boto3 sagemaker scikit-learn pandas numpy joblib matplotlib seaborn -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS Model Deployment - Google Colab Compatible Examples\n",
        "# Module 9: Model Deployment on AWS Cloud\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Install Required Packages\n",
        "# =============================================================================\n",
        "# Import all required libraries\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "from io import BytesIO, StringIO\n",
        "import json\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Mock AWS Credentials Setup (For Learning Purposes)\n",
        "# =============================================================================\n",
        "\n",
        "class MockS3Client:\n",
        "    \"\"\"Mock S3 client for demonstration without real AWS credentials\"\"\"\n",
        "    def __init__(self):\n",
        "        self.buckets = {}\n",
        "        print(\"🔧 Mock S3 Client initialized\")\n",
        "\n",
        "    def create_bucket(self, Bucket, **kwargs):\n",
        "        self.buckets[Bucket] = {}\n",
        "        print(f\"✅ Mock bucket '{Bucket}' created\")\n",
        "        return {\"ResponseMetadata\": {\"HTTPStatusCode\": 200}}\n",
        "\n",
        "    def put_object(self, Bucket, Key, Body):\n",
        "        if Bucket not in self.buckets:\n",
        "            self.buckets[Bucket] = {}\n",
        "        self.buckets[Bucket][Key] = Body\n",
        "        print(f\"✅ Object uploaded to s3://{Bucket}/{Key}\")\n",
        "        return {\"ResponseMetadata\": {\"HTTPStatusCode\": 200}}\n",
        "\n",
        "    def get_object(self, Bucket, Key):\n",
        "        if Bucket in self.buckets and Key in self.buckets[Bucket]:\n",
        "            return {\"Body\": StringIO(self.buckets[Bucket][Key])}\n",
        "        else:\n",
        "            raise Exception(f\"Object s3://{Bucket}/{Key} not found\")\n",
        "\n",
        "    def list_objects_v2(self, Bucket, **kwargs):\n",
        "        if Bucket in self.buckets:\n",
        "            contents = [{\"Key\": key} for key in self.buckets[Bucket].keys()]\n",
        "            return {\"Contents\": contents}\n",
        "        return {\"Contents\": []}\n",
        "\n",
        "# Initialize mock S3 client\n",
        "s3_client = MockS3Client()\n",
        "\n",
        "# =============================================================================\n",
        "# 3. S3 Data Storage Examples (Mock Implementation)\n",
        "# =============================================================================\n",
        "\n",
        "def create_sample_dataset():\n",
        "    \"\"\"Create sample dataset for demonstration\"\"\"\n",
        "    print(\"📊 Creating sample dataset...\")\n",
        "\n",
        "    # Generate synthetic classification dataset\n",
        "    X, y = make_classification(\n",
        "        n_samples=1000,\n",
        "        n_features=4,\n",
        "        n_classes=2,\n",
        "        n_informative=3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3', 'feature4'])\n",
        "    df['target'] = y\n",
        "\n",
        "    print(f\"✅ Dataset created: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "    print(\"\\n📈 Dataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\n📊 Target Distribution:\")\n",
        "    print(df['target'].value_counts())\n",
        "\n",
        "    # Visualize dataset\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    fig.suptitle('Dataset Overview', fontsize=16)\n",
        "\n",
        "    # Feature distributions\n",
        "    for i, col in enumerate(['feature1', 'feature2', 'feature3', 'feature4']):\n",
        "        ax = axes[i//2, i%2]\n",
        "        df.boxplot(column=col, by='target', ax=ax)\n",
        "        ax.set_title(f'{col} by Target')\n",
        "\n",
        "    # Correlation heatmap\n",
        "    ax = axes[1, 2]\n",
        "    correlation_matrix = df.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=ax)\n",
        "    ax.set_title('Feature Correlation')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "def upload_csv_to_s3_mock(df, bucket_name, key):\n",
        "    \"\"\"Upload DataFrame to mock S3 as CSV\"\"\"\n",
        "    csv_content = df.to_csv(index=False)\n",
        "    s3_client.put_object(Bucket=bucket_name, Key=key, Body=csv_content)\n",
        "    return f\"s3://{bucket_name}/{key}\"\n",
        "\n",
        "def upload_model_to_s3_mock(model, bucket_name, key):\n",
        "    \"\"\"Upload trained model to mock S3\"\"\"\n",
        "    model_buffer = BytesIO()\n",
        "    joblib.dump(model, model_buffer)\n",
        "    model_content = model_buffer.getvalue()\n",
        "    s3_client.put_object(Bucket=bucket_name, Key=key, Body=model_content)\n",
        "    return f\"s3://{bucket_name}/{key}\"\n",
        "\n",
        "def download_csv_from_s3_mock(bucket_name, key):\n",
        "    \"\"\"Download CSV from mock S3\"\"\"\n",
        "    try:\n",
        "        obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
        "        df = pd.read_csv(obj['Body'])\n",
        "        print(f\"✅ Downloaded CSV from s3://{bucket_name}/{key}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_model_from_s3_mock(bucket_name, key):\n",
        "    \"\"\"Download model from mock S3\"\"\"\n",
        "    try:\n",
        "        # For demonstration, we'll return a newly trained model\n",
        "        print(f\"✅ Downloaded model from s3://{bucket_name}/{key}\")\n",
        "        # Return a simple model for demo\n",
        "        X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "        model.fit(X, y)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading model: {e}\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Model Training and Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "def train_and_evaluate_model(df):\n",
        "    \"\"\"Train and evaluate machine learning model\"\"\"\n",
        "    print(\"🤖 Training machine learning model...\")\n",
        "\n",
        "    # Prepare data\n",
        "    X = df[['feature1', 'feature2', 'feature3', 'feature4']]\n",
        "    y = df['target']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"📊 Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"📊 Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Train model\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"✅ Model training completed\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n📈 Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\n🎯 Feature Importance:\")\n",
        "    print(feature_importance)\n",
        "\n",
        "    # Visualize results\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Feature importance plot\n",
        "    axes[0].barh(feature_importance['feature'], feature_importance['importance'])\n",
        "    axes[0].set_title('Feature Importance')\n",
        "    axes[0].set_xlabel('Importance')\n",
        "\n",
        "    # Prediction distribution\n",
        "    axes[1].hist(y_pred_proba[:, 1], bins=20, alpha=0.7, edgecolor='black')\n",
        "    axes[1].set_title('Prediction Probability Distribution')\n",
        "    axes[1].set_xlabel('Probability of Class 1')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model, accuracy\n",
        "\n",
        "# =============================================================================\n",
        "# 5. AWS EC2 Simulation (Mock Implementation)\n",
        "# =============================================================================\n",
        "\n",
        "class MockEC2:\n",
        "    \"\"\"Mock EC2 service for demonstration\"\"\"\n",
        "    def __init__(self):\n",
        "        self.instances = {}\n",
        "        print(\"🔧 Mock EC2 service initialized\")\n",
        "\n",
        "    def create_instances(self, **kwargs):\n",
        "        instance_id = f\"i-{np.random.randint(100000, 999999)}\"\n",
        "        self.instances[instance_id] = {\n",
        "            'ImageId': kwargs.get('ImageId', 'ami-12345678'),\n",
        "            'InstanceType': kwargs.get('InstanceType', 'ml.m5.large'),\n",
        "            'State': 'running'\n",
        "        }\n",
        "        print(f\"✅ EC2 Instance created: {instance_id}\")\n",
        "        print(f\"   Instance Type: {kwargs.get('InstanceType')}\")\n",
        "        print(f\"   Image ID: {kwargs.get('ImageId')}\")\n",
        "        return [MockInstance(instance_id)]\n",
        "\n",
        "class MockInstance:\n",
        "    def __init__(self, instance_id):\n",
        "        self.id = instance_id\n",
        "\n",
        "def simulate_ec2_deep_learning_setup():\n",
        "    \"\"\"Simulate EC2 deep learning instance setup\"\"\"\n",
        "    print(\"🚀 Simulating EC2 Deep Learning Instance Setup\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    ec2 = MockEC2()\n",
        "\n",
        "    # Simulate instance creation\n",
        "    instances = ec2.create_instances(\n",
        "        ImageId='ami-0c02fb55956c7d316',  # Deep Learning AMI\n",
        "        InstanceType='p3.2xlarge',  # GPU instance\n",
        "        MinCount=1,\n",
        "        MaxCount=1\n",
        "    )\n",
        "\n",
        "    instance_id = instances[0].id\n",
        "\n",
        "    # Simulate training script execution\n",
        "    print(f\"\\n📝 Executing training script on instance {instance_id}...\")\n",
        "    print(\"   - Installing dependencies...\")\n",
        "    print(\"   - Downloading data from S3...\")\n",
        "    print(\"   - Training model...\")\n",
        "    print(\"   - Uploading results to S3...\")\n",
        "    print(\"✅ Training completed successfully!\")\n",
        "\n",
        "    return instance_id\n",
        "\n",
        "# =============================================================================\n",
        "# 6. SageMaker Simulation (Mock Implementation)\n",
        "# =============================================================================\n",
        "\n",
        "class MockSageMaker:\n",
        "    \"\"\"Mock SageMaker service for demonstration\"\"\"\n",
        "    def __init__(self):\n",
        "        self.training_jobs = {}\n",
        "        self.endpoints = {}\n",
        "        print(\"🔧 Mock SageMaker service initialized\")\n",
        "\n",
        "    def create_training_job(self, job_name, algorithm_spec, input_data):\n",
        "        job_id = f\"sm-training-{np.random.randint(1000, 9999)}\"\n",
        "        self.training_jobs[job_id] = {\n",
        "            'JobName': job_name,\n",
        "            'JobStatus': 'Completed',\n",
        "            'AlgorithmSpecification': algorithm_spec,\n",
        "            'ModelArtifacts': f's3://sagemaker-models/{job_id}/model.tar.gz'\n",
        "        }\n",
        "        print(f\"✅ SageMaker training job created: {job_id}\")\n",
        "        return job_id\n",
        "\n",
        "    def create_endpoint(self, endpoint_name, model_name):\n",
        "        endpoint_id = f\"sm-endpoint-{np.random.randint(1000, 9999)}\"\n",
        "        self.endpoints[endpoint_id] = {\n",
        "            'EndpointName': endpoint_name,\n",
        "            'EndpointStatus': 'InService',\n",
        "            'ModelName': model_name\n",
        "        }\n",
        "        print(f\"✅ SageMaker endpoint created: {endpoint_id}\")\n",
        "        return endpoint_id\n",
        "\n",
        "def simulate_sagemaker_training():\n",
        "    \"\"\"Simulate SageMaker training job\"\"\"\n",
        "    print(\"🎯 Simulating SageMaker Training Job\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    sagemaker = MockSageMaker()\n",
        "\n",
        "    # Create training job\n",
        "    job_id = sagemaker.create_training_job(\n",
        "        job_name='ml-classification-training',\n",
        "        algorithm_spec={\n",
        "            'TrainingImage': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'\n",
        "        },\n",
        "        input_data='s3://my-bucket/training-data/'\n",
        "    )\n",
        "\n",
        "    print(f\"📊 Training Progress:\")\n",
        "    print(\"   [████████████████████] 100% Complete\")\n",
        "    print(\"   - Data preprocessing: ✅\")\n",
        "    print(\"   - Model training: ✅\")\n",
        "    print(\"   - Model evaluation: ✅\")\n",
        "    print(\"   - Artifact upload: ✅\")\n",
        "\n",
        "    return job_id\n",
        "\n",
        "def simulate_sagemaker_deployment(model_name):\n",
        "    \"\"\"Simulate SageMaker model deployment\"\"\"\n",
        "    print(\"🚀 Simulating SageMaker Model Deployment\")\n",
        "    print(\"=\"*42)\n",
        "\n",
        "    sagemaker = MockSageMaker()\n",
        "\n",
        "    # Create endpoint\n",
        "    endpoint_id = sagemaker.create_endpoint(\n",
        "        endpoint_name='ml-classification-endpoint',\n",
        "        model_name=model_name\n",
        "    )\n",
        "\n",
        "    print(f\"🌐 Endpoint Status: InService\")\n",
        "    print(f\"🔗 Endpoint URL: https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/{endpoint_id}/invocations\")\n",
        "\n",
        "    return endpoint_id\n",
        "\n",
        "def simulate_sagemaker_prediction(endpoint_id, test_data):\n",
        "    \"\"\"Simulate making predictions via SageMaker endpoint\"\"\"\n",
        "    print(\"🔮 Simulating SageMaker Predictions\")\n",
        "    print(\"=\"*35)\n",
        "\n",
        "    # Mock predictions\n",
        "    predictions = np.random.choice([0, 1], size=len(test_data))\n",
        "    probabilities = np.random.rand(len(test_data))\n",
        "\n",
        "    results = pd.DataFrame({\n",
        "        'prediction': predictions,\n",
        "        'probability': probabilities\n",
        "    })\n",
        "\n",
        "    print(f\"📊 Processed {len(test_data)} predictions:\")\n",
        "    print(results.head())\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Hyperparameter Tuning Simulation\n",
        "# =============================================================================\n",
        "\n",
        "def simulate_hyperparameter_tuning():\n",
        "    \"\"\"Simulate SageMaker hyperparameter tuning\"\"\"\n",
        "    print(\"🎛️ Simulating Hyperparameter Tuning Job\")\n",
        "    print(\"=\"*42)\n",
        "\n",
        "    # Define hyperparameter space\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "\n",
        "    print(\"🔧 Hyperparameter Search Space:\")\n",
        "    for param, values in param_grid.items():\n",
        "        print(f\"   {param}: {values}\")\n",
        "\n",
        "    # Simulate tuning results\n",
        "    results = []\n",
        "    for i in range(6):  # Simulate 6 training jobs\n",
        "        config = {\n",
        "            'n_estimators': np.random.choice(param_grid['n_estimators']),\n",
        "            'max_depth': np.random.choice(param_grid['max_depth']),\n",
        "            'min_samples_split': np.random.choice(param_grid['min_samples_split'])\n",
        "        }\n",
        "        accuracy = 0.8 + np.random.random() * 0.15  # Random accuracy between 0.8-0.95\n",
        "\n",
        "        results.append({\n",
        "            'job_id': f'tuning-job-{i+1:03d}',\n",
        "            'accuracy': accuracy,\n",
        "            **config\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results).sort_values('accuracy', ascending=False)\n",
        "\n",
        "    print(\"\\n📈 Tuning Results (Top 6 configurations):\")\n",
        "    print(results_df)\n",
        "\n",
        "    best_config = results_df.iloc[0]\n",
        "    print(f\"\\n🏆 Best Configuration:\")\n",
        "    print(f\"   Accuracy: {best_config['accuracy']:.4f}\")\n",
        "    print(f\"   Parameters: {dict(best_config.drop(['job_id', 'accuracy']))}\")\n",
        "\n",
        "    return best_config\n",
        "\n",
        "# =============================================================================\n",
        "# 8. AWS Lambda Simulation\n",
        "# =============================================================================\n",
        "\n",
        "def simulate_lambda_function(input_data):\n",
        "    \"\"\"Simulate AWS Lambda serverless inference\"\"\"\n",
        "    print(\"⚡ Simulating AWS Lambda Serverless Inference\")\n",
        "    print(\"=\"*48)\n",
        "\n",
        "    print(\"🔄 Lambda function execution:\")\n",
        "    print(\"   - Cold start: 250ms\")\n",
        "    print(\"   - Loading model from S3: 150ms\")\n",
        "    print(\"   - Making prediction: 50ms\")\n",
        "    print(\"   - Total execution time: 450ms\")\n",
        "\n",
        "    # Mock prediction\n",
        "    prediction = np.random.choice([0, 1])\n",
        "    probability = np.random.rand()\n",
        "\n",
        "    response = {\n",
        "        'statusCode': 200,\n",
        "        'body': {\n",
        "            'prediction': int(prediction),\n",
        "            'probability': float(probability),\n",
        "            'execution_time_ms': 450,\n",
        "            'model_version': 'v1.2.3'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"✅ Lambda Response:\")\n",
        "    print(f\"   Status Code: {response['statusCode']}\")\n",
        "    print(f\"   Prediction: {response['body']['prediction']}\")\n",
        "    print(f\"   Probability: {response['body']['probability']:.4f}\")\n",
        "    print(f\"   Execution Time: {response['body']['execution_time_ms']}ms\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# =============================================================================\n",
        "# 9. Complete ML Pipeline Demonstration\n",
        "# =============================================================================\n",
        "\n",
        "def run_complete_ml_pipeline():\n",
        "    \"\"\"Run complete ML pipeline demonstration\"\"\"\n",
        "    print(\"🚀 COMPLETE AWS ML PIPELINE DEMONSTRATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Create and explore dataset\n",
        "    print(\"\\n📊 STEP 1: Data Preparation\")\n",
        "    print(\"-\" * 30)\n",
        "    df = create_sample_dataset()\n",
        "\n",
        "    # Step 2: Upload data to S3 (mock)\n",
        "    print(\"\\n☁️ STEP 2: Upload Data to S3\")\n",
        "    print(\"-\" * 30)\n",
        "    s3_client.create_bucket(Bucket='ml-demo-bucket')\n",
        "    upload_csv_to_s3_mock(df, 'ml-demo-bucket', 'data/training_data.csv')\n",
        "\n",
        "    # Step 3: Train model locally\n",
        "    print(\"\\n🤖 STEP 3: Model Training & Evaluation\")\n",
        "    print(\"-\" * 38)\n",
        "    model, accuracy = train_and_evaluate_model(df)\n",
        "\n",
        "    # Step 4: Upload model to S3 (mock)\n",
        "    print(\"\\n📤 STEP 4: Upload Model to S3\")\n",
        "    print(\"-\" * 28)\n",
        "    upload_model_to_s3_mock(model, 'ml-demo-bucket', 'models/rf_model.pkl')\n",
        "\n",
        "    # Step 5: EC2 training simulation\n",
        "    print(\"\\n🖥️ STEP 5: EC2 Deep Learning Setup\")\n",
        "    print(\"-\" * 33)\n",
        "    instance_id = simulate_ec2_deep_learning_setup()\n",
        "\n",
        "    # Step 6: SageMaker training\n",
        "    print(\"\\n🎯 STEP 6: SageMaker Training\")\n",
        "    print(\"-\" * 28)\n",
        "    job_id = simulate_sagemaker_training()\n",
        "\n",
        "    # Step 7: Hyperparameter tuning\n",
        "    print(\"\\n🎛️ STEP 7: Hyperparameter Tuning\")\n",
        "    print(\"-\" * 31)\n",
        "    best_config = simulate_hyperparameter_tuning()\n",
        "\n",
        "    # Step 8: Model deployment\n",
        "    print(\"\\n🚀 STEP 8: Model Deployment\")\n",
        "    print(\"-\" * 26)\n",
        "    endpoint_id = simulate_sagemaker_deployment('optimized-rf-model')\n",
        "\n",
        "    # Step 9: Make predictions\n",
        "    print(\"\\n🔮 STEP 9: Model Inference\")\n",
        "    print(\"-\" * 24)\n",
        "    test_data = df.sample(5)[['feature1', 'feature2', 'feature3', 'feature4']]\n",
        "    predictions = simulate_sagemaker_prediction(endpoint_id, test_data)\n",
        "\n",
        "    # Step 10: Lambda serverless inference\n",
        "    print(\"\\n⚡ STEP 10: Serverless Inference\")\n",
        "    print(\"-\" * 30)\n",
        "    lambda_response = simulate_lambda_function([1.5, 2.3, 0.8, 1.2])\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n🎉 PIPELINE COMPLETION SUMMARY\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"✅ Dataset: {df.shape[0]} samples, {df.shape[1]-1} features\")\n",
        "    print(f\"✅ Model Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"✅ S3 Objects: 2 (data + model)\")\n",
        "    print(f\"✅ EC2 Instance: {instance_id}\")\n",
        "    print(f\"✅ SageMaker Job: {job_id}\")\n",
        "    print(f\"✅ Best HP Config: {best_config['accuracy']:.4f} accuracy\")\n",
        "    print(f\"✅ Endpoint: {endpoint_id}\")\n",
        "    print(f\"✅ Predictions: {len(predictions)} samples\")\n",
        "    print(f\"✅ Lambda: 450ms response time\")\n",
        "\n",
        "    return {\n",
        "        'dataset_size': df.shape[0],\n",
        "        'model_accuracy': accuracy,\n",
        "        'instance_id': instance_id,\n",
        "        'job_id': job_id,\n",
        "        'endpoint_id': endpoint_id,\n",
        "        'predictions': predictions\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 10. Monitoring and Cost Optimization\n",
        "# =============================================================================\n",
        "\n",
        "def simulate_monitoring_dashboard():\n",
        "    \"\"\"Simulate CloudWatch monitoring dashboard\"\"\"\n",
        "    print(\"📊 AWS CloudWatch Monitoring Dashboard\")\n",
        "    print(\"=\"*45)\n",
        "\n",
        "    # Generate mock metrics\n",
        "    timestamps = pd.date_range('2024-01-01', periods=24, freq='H')\n",
        "    metrics_data = {\n",
        "        'timestamp': timestamps,\n",
        "        'prediction_count': np.random.poisson(50, 24),\n",
        "        'latency_ms': np.random.normal(200, 50, 24),\n",
        "        'error_rate': np.random.beta(1, 99, 24) * 100,\n",
        "        'cpu_utilization': np.random.normal(60, 15, 24)\n",
        "    }\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    # Create monitoring plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('AWS ML Model Monitoring Dashboard', fontsize=16)\n",
        "\n",
        "    # Prediction volume\n",
        "    axes[0, 0].plot(metrics_df['timestamp'], metrics_df['prediction_count'])\n",
        "    axes[0, 0].set_title('Prediction Volume')\n",
        "    axes[0, 0].set_ylabel('Predictions/Hour')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Response latency\n",
        "    axes[0, 1].plot(metrics_df['timestamp'], metrics_df['latency_ms'], color='orange')\n",
        "    axes[0, 1].set_title('Response Latency')\n",
        "    axes[0, 1].set_ylabel('Latency (ms)')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Error rate\n",
        "    axes[1, 0].plot(metrics_df['timestamp'], metrics_df['error_rate'], color='red')\n",
        "    axes[1, 0].set_title('Error Rate')\n",
        "    axes[1, 0].set_ylabel('Error Rate (%)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # CPU utilization\n",
        "    axes[1, 1].plot(metrics_df['timestamp'], metrics_df['cpu_utilization'], color='green')\n",
        "    axes[1, 1].set_title('CPU Utilization')\n",
        "    axes[1, 1].set_ylabel('CPU %')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\n📈 24-Hour Metrics Summary:\")\n",
        "    print(f\"   Total Predictions: {metrics_df['prediction_count'].sum():,}\")\n",
        "    print(f\"   Avg Latency: {metrics_df['latency_ms'].mean():.1f}ms\")\n",
        "    print(f\"   Max Error Rate: {metrics_df['error_rate'].max():.2f}%\")\n",
        "    print(f\"   Avg CPU Usage: {metrics_df['cpu_utilization'].mean():.1f}%\")\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "def cost_optimization_analysis():\n",
        "    \"\"\"Analyze cost optimization opportunities\"\"\"\n",
        "    print(\"💰 AWS Cost Optimization Analysis\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Mock cost data\n",
        "    services = ['EC2', 'SageMaker', 'S3', 'Lambda', 'CloudWatch']\n",
        "    monthly_costs = [250, 180, 15, 8, 12]\n",
        "\n",
        "    cost_df = pd.DataFrame({\n",
        "        'service': services,\n",
        "        'monthly_cost': monthly_costs\n",
        "    })\n",
        "\n",
        "    # Cost breakdown visualization\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.pie(cost_df['monthly_cost'], labels=cost_df['service'], autopct='%1.1f%%')\n",
        "    plt.title('Monthly AWS Costs by Service')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(cost_df['service'], cost_df['monthly_cost'], color='skyblue')\n",
        "    plt.title('Monthly Cost Breakdown')\n",
        "    plt.ylabel('Cost ($)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    total_cost = cost_df['monthly_cost'].sum()\n",
        "    print(f\"\\n💲 Total Monthly Cost: ${total_cost}\")\n",
        "    print(\"\\n🔧 Optimization Recommendations:\")\n",
        "    print(\"   1. Use Spot Instances for EC2: Save up to 70%\")\n",
        "    print(\"   2. Enable SageMaker Auto Scaling: Save 20-40%\")\n",
        "    print(\"   3. Use S3 Intelligent Tiering: Save 10-20%\")\n",
        "    print(\"   4. Optimize Lambda memory allocation: Save 15%\")\n",
        "\n",
        "    return cost_df\n",
        "\n",
        "# =============================================================================\n",
        "# 11. Main Execution Function\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run all demonstrations\"\"\"\n",
        "    print(\"🎯 AWS ML DEPLOYMENT - GOOGLE COLAB DEMO\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"This notebook demonstrates AWS ML deployment concepts\")\n",
        "    print(\"using mock implementations that run in Google Colab\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # Run complete pipeline\n",
        "        results = run_complete_ml_pipeline()\n",
        "\n",
        "        # Show monitoring dashboard\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        metrics_df = simulate_monitoring_dashboard()\n",
        "\n",
        "        # Cost analysis\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        cost_df = cost_optimization_analysis()\n",
        "\n",
        "        print(\"\\n🎉 ALL DEMONSTRATIONS COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"✅ You've learned about:\")\n",
        "        print(\"   • S3 data storage and management\")\n",
        "        print(\"   • EC2 deep learning instances\")\n",
        "        print(\"   • SageMaker training and deployment\")\n",
        "        print(\"   • Hyperparameter tuning\")\n",
        "        print(\"   • Serverless inference with Lambda\")\n",
        "        print(\"   • Model monitoring and cost optimization\")\n",
        "\n",
        "        return results, metrics_df, cost_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in demonstration: {e}\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# 12. Interactive Learning Functions\n",
        "# =============================================================================\n",
        "\n",
        "def aws_quiz():\n",
        "    \"\"\"Interactive quiz about AWS ML concepts\"\"\"\n",
        "    print(\"🧠 AWS ML DEPLOYMENT QUIZ\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    questions = [\n",
        "        {\n",
        "            \"question\": \"Which AWS service is best for storing training data?\",\n",
        "            \"options\": [\"A) EC2\", \"B) S3\", \"C) Lambda\", \"D) RDS\"],\n",
        "            \"answer\": \"B\",\n",
        "            \"explanation\": \"S3 is designed for object storage and is perfect for ML datasets\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What type of EC2 instance is best for deep learning?\",\n",
        "            \"options\": [\"A) t2.micro\", \"B) m5.large\", \"C) p3.2xlarge\", \"D) c5.large\"],\n",
        "            \"answer\": \"C\",\n",
        "            \"explanation\": \"p3 instances have GPUs optimized for machine learning workloads\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which service provides managed ML model training?\",\n",
        "            \"options\": [\"A) EC2\", \"B) Lambda\", \"C) S3\", \"D) SageMaker\"],\n",
        "            \"answer\": \"D\",\n",
        "            \"explanation\": \"SageMaker is AWS's managed machine learning service\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    for i, q in enumerate(questions, 1):\n",
        "        print(f\"\\n❓ Question {i}: {q['question']}\")\n",
        "        for option in q['options']:\n",
        "            print(f\"   {option}\")\n",
        "\n",
        "        # In a real interactive environment, you'd get user input\n",
        "        # For demo, we'll show the answer\n",
        "        print(f\"✅ Correct Answer: {q['answer']}\")\n",
        "        print(f\"💡 Explanation: {q['explanation']}\")\n",
        "        score += 1\n",
        "\n",
        "    print(f\"\\n🎉 Quiz Complete! Score: {score}/{len(questions)}\")\n",
        "\n",
        "# Run the main demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    # Execute all demonstrations\n",
        "    results = main()\n",
        "\n",
        "    # Run quiz\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    aws_quiz()\n",
        "\n",
        "    print(\"\\n🎓 LEARNING COMPLETE!\")\n",
        "    print(\"You now have hands-on experience with AWS ML deployment concepts!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbsZwriTWuwg",
        "outputId": "225364bd-3e7c-4219-d36b-815a837953a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All packages installed successfully!\n",
            "🔧 Mock S3 Client initialized\n",
            "🎯 AWS ML DEPLOYMENT - GOOGLE COLAB DEMO\n",
            "============================================================\n",
            "This notebook demonstrates AWS ML deployment concepts\n",
            "using mock implementations that run in Google Colab\n",
            "============================================================\n",
            "🚀 COMPLETE AWS ML PIPELINE DEMONSTRATION\n",
            "============================================================\n",
            "\n",
            "📊 STEP 1: Data Preparation\n",
            "------------------------------\n",
            "📊 Creating sample dataset...\n",
            "❌ Error in demonstration: Number of informative, redundant and repeated features must sum to less than the number of total features\n",
            "\n",
            "============================================================\n",
            "🧠 AWS ML DEPLOYMENT QUIZ\n",
            "==============================\n",
            "\n",
            "❓ Question 1: Which AWS service is best for storing training data?\n",
            "   A) EC2\n",
            "   B) S3\n",
            "   C) Lambda\n",
            "   D) RDS\n",
            "✅ Correct Answer: B\n",
            "💡 Explanation: S3 is designed for object storage and is perfect for ML datasets\n",
            "\n",
            "❓ Question 2: What type of EC2 instance is best for deep learning?\n",
            "   A) t2.micro\n",
            "   B) m5.large\n",
            "   C) p3.2xlarge\n",
            "   D) c5.large\n",
            "✅ Correct Answer: C\n",
            "💡 Explanation: p3 instances have GPUs optimized for machine learning workloads\n",
            "\n",
            "❓ Question 3: Which service provides managed ML model training?\n",
            "   A) EC2\n",
            "   B) Lambda\n",
            "   C) S3\n",
            "   D) SageMaker\n",
            "✅ Correct Answer: D\n",
            "💡 Explanation: SageMaker is AWS's managed machine learning service\n",
            "\n",
            "🎉 Quiz Complete! Score: 3/3\n",
            "\n",
            "🎓 LEARNING COMPLETE!\n",
            "You now have hands-on experience with AWS ML deployment concepts!\n"
          ]
        }
      ]
    }
  ]
}